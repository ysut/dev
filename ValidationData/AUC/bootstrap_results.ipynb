{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "########   Initialize and setup pandas methods   ########\n",
    "pandarallel.initialize(nb_workers=os.cpu_count()-1, progress_bar=False, \n",
    "                       verbose=2, use_memory_fs=False) \n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp' \n",
    "\n",
    "try: \n",
    "    __file__\n",
    "    sys.path.append(os.path.join(os.path.dirname('__file__')))\n",
    "except NameError:\n",
    "    Path().resolve()\n",
    "    sys.path.append(os.path.join(Path().resolve(), '../../'))\n",
    "\n",
    "from libs.scoring import Scoring\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the 4,850 solutions\n",
    "fp_solution_pkl = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/pkls/all_solution_main.pkl\"\n",
    "with open(fp_solution_pkl, 'rb') as f:\n",
    "    all_solutions = pickle.load(f)\n",
    "\n",
    "solutions = pd.DataFrame(all_solutions)\n",
    "solutions_list = [f\"Solution {i}\" for i in range(1, 4851)]\n",
    "patterns_sr = pd.Series(solutions_list, name='Solution')\n",
    "solutions = pd.concat([patterns_sr, solutions], axis=1)\n",
    "\n",
    "# Load pkls (all_results_1_10.pkl, all_results_11_20.pkl, ..., all_results_91_100.pkl)\n",
    "results_1_10 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_1_10.pkl\"\n",
    "results_11_20 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_11_20.pkl\"\n",
    "results_21_30 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_21_30.pkl\"\n",
    "results_31_40 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_31_40.pkl\"\n",
    "results_41_50 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_41_50.pkl\"\n",
    "results_51_60 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_51_60.pkl\"\n",
    "results_61_70 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_61_70.pkl\"\n",
    "results_71_80 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_71_80.pkl\"\n",
    "results_81_90 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_81_90.pkl\"\n",
    "results_91_100 = \"/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/all_results_91_100.pkl\"\n",
    "\n",
    "# Load pickles of calculated AUCs\n",
    "with open(results_1_10, 'rb') as f:\n",
    "    dict_1_10 = pickle.load(f)\n",
    "\n",
    "with open(results_11_20, 'rb') as f:\n",
    "    dict_11_20 = pickle.load(f)\n",
    "\n",
    "with open(results_21_30, 'rb') as f:\n",
    "    dict_21_30 = pickle.load(f)\n",
    "\n",
    "with open(results_31_40, 'rb') as f:\n",
    "    dict_31_40 = pickle.load(f)\n",
    "\n",
    "with open(results_41_50, 'rb') as f:\n",
    "    dict_41_50 = pickle.load(f)\n",
    "\n",
    "with open(results_51_60, 'rb') as f:\n",
    "    dict_51_60 = pickle.load(f)\n",
    "\n",
    "with open(results_61_70, 'rb') as f:\n",
    "    dict_61_70 = pickle.load(f)\n",
    "\n",
    "with open(results_71_80, 'rb') as f:\n",
    "    dict_71_80 = pickle.load(f)\n",
    "\n",
    "with open(results_81_90, 'rb') as f:\n",
    "    dict_81_90 = pickle.load(f)\n",
    "\n",
    "with open(results_91_100, 'rb') as f:\n",
    "    dict_91_100 = pickle.load(f)\n",
    "\n",
    "# To dataframes\n",
    "df_1_10 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_1_10.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_1_10[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_1_10 = pd.concat([df_1_10, buf_df], ignore_index=True)\n",
    "\n",
    "df_11_20 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_11_20.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_11_20[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_11_20 = pd.concat([df_11_20, buf_df], ignore_index=True)\n",
    "\n",
    "df_21_30 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_21_30.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_21_30[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_21_30 = pd.concat([df_21_30, buf_df], ignore_index=True)\n",
    "\n",
    "df_31_40 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_31_40.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_31_40[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_31_40 = pd.concat([df_31_40, buf_df], ignore_index=True)\n",
    "\n",
    "df_41_50 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_41_50.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_41_50[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_41_50 = pd.concat([df_41_50, buf_df], ignore_index=True)\n",
    "\n",
    "df_51_60 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_51_60.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_51_60[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_51_60 = pd.concat([df_51_60, buf_df], ignore_index=True)\n",
    "\n",
    "df_61_70 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_61_70.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_61_70[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_61_70 = pd.concat([df_61_70, buf_df], ignore_index=True)\n",
    "\n",
    "df_71_80 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_71_80.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_71_80[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_71_80 = pd.concat([df_71_80, buf_df], ignore_index=True)\n",
    "\n",
    "df_81_90 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_81_90.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_81_90[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_81_90 = pd.concat([df_81_90, buf_df], ignore_index=True)\n",
    "\n",
    "df_91_100 = pd.DataFrame(columns=['Dataset', 'Candidate', 'Score'])\n",
    "for set in dict_91_100.keys():\n",
    "    buf_df = pd.DataFrame(\n",
    "        list(dict_91_100[set].items()), columns=['Candidate', 'Score'])\n",
    "    buf_df['Dataset'] = set\n",
    "    df_91_100 = pd.concat([df_91_100, buf_df], ignore_index=True)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df_all = pd.concat([df_1_10, df_11_20, df_21_30, df_31_40, df_41_50, df_51_60, df_61_70, df_71_80, df_81_90, df_91_100], ignore_index=True)\n",
    "\n",
    "# Split the 'Score' column into 'auROC' and '95%CI'\n",
    "df_all['auROC'] = df_all['Score'].str.split(' ', expand=True)[0].astype(float)\n",
    "df_all['95%CI'] = df_all['Score'].str.split(' ', expand=True)[1]\n",
    "df_all['CI_lower'] = df_all['95%CI'].str.extract(r'(\\d\\.\\d*)').astype('float')\n",
    "df_all['CI_upper'] = df_all['95%CI'].str.extract(r'((?<=-)\\d\\.\\d*(?=]))').astype(float)\n",
    "df_all.drop(['Score', '95%CI'], axis=1, inplace=True)\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = pd.merge(df_all, solutions, left_on='Candidate', right_on='Solution')\n",
    "df.drop('Solution', axis=1, inplace=True)\n",
    "\n",
    "# Calculate the sample variance of the scores for each solution\n",
    "df['SampleVariance'] = df.iloc[:, 5:20].var(axis=1, ddof=0)\n",
    "\n",
    "# Calculate the maximum scores for each solution\n",
    "ddf = df.groupby('Dataset')\n",
    "maxdf = df.loc[ddf['auROC'].idxmax(),:]\n",
    "\n",
    "# Replace 'Solution' to 'Pattern' in the 'Candidate' column\n",
    "df['Candidate'] = df['Candidate'].str.replace('Solution', 'Pattern')\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'Dataset': 'No. of Bootstrap',\n",
    "                   'CI_lower': '95%_CI_lower', 'CI_upper': '95%_CI_upper',\n",
    "                   'SampleVariance': 'Sample Variance',\n",
    "                   's1': 'Score 1', 's2': 'Score 2', 's3': 'Score 3', \n",
    "                   's4': 'Score 4', 's5': 'Score 5', 's6': 'Score 6', \n",
    "                   's7': 'Score 7', 's8': 'Score 8', 's9': 'Score 9', \n",
    "                   's10': 'Score 10', 's11': 'Score 11', 's12': 'Score 12', \n",
    "                   's13': 'Score 13', 's14': 'Score 14'}, inplace=True)\n",
    "\n",
    "bootstrap = 100\n",
    "set_max = []\n",
    "for i in range(1, bootstrap + 1):\n",
    "    set_max.append(df.loc[(df['No. of Bootstrap'] == i) & (df['auROC'] == maxdf.loc[maxdf['Dataset'] == i, 'auROC'].values[0]), :])\n",
    "\n",
    "best = {}\n",
    "for i in range(bootstrap):\n",
    "    highest_variance = set_max[i].loc[set_max[i]['Sample Variance'].idxmax(), 'Sample Variance']\n",
    "    # If other candidate has the same variance, add it to the \n",
    "    best[f'set {i + 1}'] = set_max[i].loc[set_max[i]['Sample Variance'] == highest_variance, 'Candidate'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output set_max\n",
    "set_max_df = pd.concat(set_max)\n",
    "set_max_df = set_max_df[['No. of Bootstrap', 'Candidate', \n",
    "                         'Score 1', 'Score 2', 'Score 3', 'Score 4', 'Score 5',\n",
    "                         'Score 6', 'Score 7', 'Score 8', 'Score 9', 'Score 10',\n",
    "                         'Score 11', 'Score 12', 'Score 13', 'Score 14',\n",
    "                         'auROC', '95%_CI_lower', '95%_CI_upper', \n",
    "                         'Sample Variance']]\n",
    "# To xlsx\n",
    "set_max_df.to_excel('/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/set_max.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output solutions as xlsx\n",
    "solutions.to_excel('/Volumes/vol/work/Github/dev/ValidationData/AUC/bootstrap_results/solutions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximumu auROCs\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add vertical lines between CI upper and lower by set\n",
    "for i in range(1, 101):\n",
    "    fig.add_shape(\n",
    "        dict(\n",
    "            type='line',\n",
    "            x0=i,\n",
    "            y0=maxdf.loc[maxdf['Dataset'] == i, 'CI_upper'].values[0],\n",
    "            x1=i,\n",
    "            y1=maxdf.loc[maxdf['Dataset'] == i, 'CI_lower'].values[0],\n",
    "            line=dict(color='red', width=1.5)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Plot each 95% CI using error bars\n",
    "fig.add_trace(go.Scatter(x=maxdf['Dataset'], y=maxdf['CI_upper'],\n",
    "                    mode='markers',\n",
    "                    name='CI upper',\n",
    "                    marker=dict(color='red', size=4)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=maxdf['Dataset'], y=maxdf['CI_lower'],\n",
    "                    mode='markers',\n",
    "                    name='CI lower',\n",
    "                    marker=dict(color='red', size=4)))\n",
    "\n",
    "# # Plot only maximum auROC\n",
    "# fig.add_trace(go.Scatter(x=maxdf['Dataset'], y=maxdf['auROC'],\n",
    "#                     mode='markers',\n",
    "#                     name='Maximum auROC',\n",
    "#                     marker=dict(color='blue', size=10)))\n",
    "\n",
    "\n",
    "# Plot the best candidate\n",
    "for i in range(bootstrap):\n",
    "    fig.add_trace(go.Scatter(x=[i+1], y=[maxdf.loc[maxdf['Dataset'] == i+1, 'auROC'].values[0]],\n",
    "                    mode='markers',\n",
    "                    name='Best candidate',\n",
    "                    marker=dict(color='red', size=8)))\n",
    "\n",
    "# plot order of lines and markers\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.update_layout(title='Maximum auROC and 95% CI using calibration data set',\n",
    "                  xaxis_title='Number of bootstrap',\n",
    "                  yaxis_title='auROC')\n",
    "# Change font size\n",
    "fig.update_layout(font=dict(size=21))\n",
    "\n",
    "# y-axis range: 0.99 to 1.00\n",
    "fig.update_yaxes(range=[0.99, 1.00])\n",
    "\n",
    "# Figure size\n",
    "fig.update_layout(width=1200, height=600)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison analysis Framework vs SpliceAI alone\n",
    "# Test data pkls\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wesanno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
